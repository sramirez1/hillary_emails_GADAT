{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####To Do's:\n",
    "1)Clean email body\n",
    "* Need to remove proper nouns\n",
    "* Need to remove any text after \"U.S. Department of State\",\"Sent from Verizon\"\n",
    "* Remove email addresses \n",
    "* Remove dates especially of the format \"Monday, January 18, 2010 11:52 AM\"\n",
    "* Remove times\n",
    "* Remove web addresses\n",
    "* Need to remove any strange non-English words\n",
    "* How do I deal with emails that list schedule for the day?\n",
    "* Change all occurences of \"pis\" to \"pls\". Assuming OCR error.\n",
    "\n",
    "2)Run K-Means Clustering\n",
    "3)Run LDA \n",
    "\n",
    "* After defining topics , determine similarity between emails for clustering. Possibly clustering texts with KMeans\n",
    "* Need to stem words? Keep a word dictionary for all stem words, they don't make sense otherwise\n",
    "* Look specifically at emails with **fwd** \n",
    "\n",
    "* Locality-sensitive hashing\n",
    "* Look into lemmatization\n",
    "* Chunking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora, models, similarities\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Setup and Review Sqlite Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Emails',), (u'Persons',), (u'Aliases',), (u'EmailReceivers',)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = sqlite3.connect('input/database.sqlite')\n",
    "cursor = con.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "[x for x in cursor.fetchall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails= pd.read_sql_query(\"Select * From Emails where ExtractedBodyText!= ''\",con)\n",
    "persons=pd.read_sql_query(\"Select * From Persons\",con)\n",
    "longemails= pd.read_sql_query(\"Select * From Emails where length(ExtractedBodyText)>500 \\\n",
    "                                and ExtractedBodyText!= ''\",con)\n",
    "aliases=pd.read_sql_query(\"Select * From Aliases\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Emails: 6742\n"
     ]
    }
   ],
   "source": [
    "print \"Number of Emails: %s\" % emails.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Clean up text of email body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the next line to download stop words if it's not already installed.\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define email_to_words function to clean email body\n",
    "#import email_to_words as email_to_words\n",
    "%run ./email_to_words.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning email 1000 of 6742\n",
      "\n",
      "Cleaning email 2000 of 6742\n",
      "\n",
      "Cleaning email 3000 of 6742\n",
      "\n",
      "Cleaning email 4000 of 6742\n",
      "\n",
      "Cleaning email 5000 of 6742\n",
      "\n",
      "Cleaning email 6000 of 6742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of emails based on the dataframe column size\n",
    "num_emails = emails[\"ExtractedBodyText\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_emails = []\n",
    "stemmed_emails = []\n",
    "# Loop over each email; create an index i that goes from 0 to the length\n",
    "# of the emails \n",
    "for i in xrange( 0, num_emails ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print \"Cleaning email %d of %d\\n\" % ( i+1, num_emails )                                                             \n",
    "  \n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    try:\n",
    "        clean_emails.append( email_to_words( emails[\"ExtractedBodyText\"][i] , english=True) )\n",
    "\n",
    "    except Exception as e:\n",
    "        clean_emails.append( email_to_words(\"I'm a placeholder sentence.\"), english=True)\n",
    "        print \"Execption raised:\", e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Tokenize and Stem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [p_stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in clean_emails:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'cleaned_emails', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 282227 items/words in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print 'There are ' + str(vocab_frame.shape[0]) + ' items/words in vocab_frame'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>voter</th>\n",
       "      <td>voter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voter</th>\n",
       "      <td>voter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voter</th>\n",
       "      <td>voter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voter</th>\n",
       "      <td>voter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voter</th>\n",
       "      <td>voter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voter</th>\n",
       "      <td>voter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voter</th>\n",
       "      <td>voter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voter</th>\n",
       "      <td>voter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voter</th>\n",
       "      <td>voter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voter</th>\n",
       "      <td>voter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voter</th>\n",
       "      <td>voter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voter</th>\n",
       "      <td>voter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       words\n",
       "voter  voter\n",
       "voter  voter\n",
       "voter  voter\n",
       "voter  voter\n",
       "voter  voter\n",
       "voter  voter\n",
       "voter  voter\n",
       "voter  voter\n",
       "voter  voter\n",
       "voter  voter\n",
       "voter  voter\n",
       "voter  voter"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_frame[vocab_frame.words==\"voter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'voted'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize(\"voted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize cleaned emails\n",
    "token_emails = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# loop through document list\n",
    "for i in clean_emails:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "   \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    token_emails.append(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "%time\n",
    "dictionary = corpora.Dictionary(token_emails)\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in token_emails]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 14.1 µs\n"
     ]
    }
   ],
   "source": [
    "# generate LDA model\n",
    "%time ldamodel = models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=30)\n",
    "\n",
    "#generate latent semantic indexing\n",
    "#lsi = models.lsimodel.LsiModel(corpus=corpus, id2word=dictionary, num_topics=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "#ldamodel.save('onlyEnglishLDA.model')\n",
    "#ldamodel.save('eng_PleaseLDA.model\n",
    "onlyEnglish=models.ldamodel.LdaModel.load('onlyEnglishLDA.model')\n",
    "engPlease=models.ldamodel.LdaModel.load('eng_PleaseLDA.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create a Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 3 µs, total: 8 µs\n",
      "Wall time: 12.2 µs\n",
      "Elayna\n",
      "Boehner\n",
      "pls\n"
     ]
    }
   ],
   "source": [
    "text=[\"Elayna\",\"food\",\"Boehner\",\"John\",\"pls\"]\n",
    "%time\n",
    "for w in text:\n",
    "    if not wordnet.synsets(w):\n",
    "        print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words=webtext.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grail=webtext.raw('overheard.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "print \"Number of words in the email corpus: %s\" % len(vocab)\n",
    "print \"Number of words in the email corpus: %s\" % len(stemvocab)\n",
    "\n",
    "print vocab[:50]\n",
    "print \"\"\n",
    "print stemvocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(stem_data_features.toarray(), axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(stemvocab, dist)[:1000]:\n",
    "    print tag, count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
